{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn import preprocessing,decomposition,metrics\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Data Loading And Preperation\n",
    "\n",
    "    -- Removing hashtags and converting to normal words, appended at end of sentence.\n",
    "    -- Adding a seperate  method for calculating multi class logloss [multiclass_logloss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_data=pd.read_csv(\"dataset/train.csv\")\n",
    "test_data=pd.read_csv(\"dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.245025e+18  Happy #MothersDay to all you amazing mothers o...   en   \n",
       "1  1.245759e+18  Happy Mothers Day Mum - I'm sorry I can't be t...   en   \n",
       "2  1.246087e+18  Happy mothers day To all This doing a mothers ...   en   \n",
       "3  1.244803e+18  Happy mothers day to this beautiful woman...ro...   en   \n",
       "4  1.244876e+18  Remembering the 3 most amazing ladies who made...   en   \n",
       "\n",
       "  retweet_count  original_author  sentiment_class  \n",
       "0             0      BeenXXPired                0  \n",
       "1             1   FestiveFeeling                0  \n",
       "2             0     KrisAllenSak               -1  \n",
       "3             0       Queenuchee                0  \n",
       "4             0  brittan17446794               -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy_metrics={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', '-0.0138325017', '-0.9677309496', '-0.3876905537',\n",
       "       '0.5309553602', '-0.045423609', '0.1210638815', '&gt',\n",
       "       ' have them delivered!', '-0.7860764746',\n",
       "       ' very much loved\\U0001f970Ô∏è ‚Ä¶', '0.7885519508', '0.4310598662',\n",
       "       '0.6034925894', '0.8837056921', '-0.4757848717', '0.1262837865',\n",
       "       '0.8296402421', '0.2203775303', '-0.0320226838', '-0.0272467108',\n",
       "       '-0.9022044897', 'I was ten weeks...', '0.4754834129', ' Ô∏è',\n",
       "       '0.7120802873', '0.7493660991', '0.3716244571', '0.4616286043',\n",
       "       '0.4479350131', ' PROSPERITY KING ZAY I MET YOU WHEN YOU WAS &amp',\n",
       "       '0.6747864639', '-0.7798220898', '-0.6668237899', '-0.948781497',\n",
       "       '-0.4189029043', '0.001710524', '0.0903948317', '-0.6936114103',\n",
       "       '0.3087589587', \" here's to !\", '-0.7182082972', '0.195401415',\n",
       "       '-0.0064143617', ' pink Peruvian opal! via', '0.8077853046',\n",
       "       '0.5129957209', '-0.1651444775', '0.4611910293',\n",
       "       ' look really confused ü§∑\\u200dÔ∏è i realize I‚Äôm ALWAYS like that!',\n",
       "       '0.8531978334',\n",
       "       \" The Best Kids' Gardening Tools For Your Little Fa..\",\n",
       "       '0.8059809024', '0.7231463898', ' Find More', '0.8612531442',\n",
       "       '-0.2725572271', '0.5134848136', '-0.3200075494', '-0.0054842701',\n",
       "       '-0.2636714548', '0.8802217512', '-0.3900960511',\n",
       "       ' Divorces Spike In China As Couples Emerge From Quarantin...',\n",
       "       ' nasty but I‚Äôm really just a younger her HAPPPY BIRTHDAY TO THE GREATEST STRONGEST LADY I KNOW Ô∏è years down more to COME /01 a Savage was born',\n",
       "       '-0.4514276457', '0.92187396', '0.0518390416', '-0.731417032',\n",
       "       '0.0806365626', '-0.3883580611', '0.3881313254', '-0.0137544943',\n",
       "       ' School jab may protect us from corona. so WHY has-',\n",
       "       '-0.401099363', '0.9992862228', '0.4981421416', '-0.4265306098',\n",
       "       '-0.3271553109', '0.3976547438', '-0.125828976', '...',\n",
       "       '0.0492668521', '0.0225309145', '0.2871981974', '0.8644551076',\n",
       "       '0.7478702688', '0.4337608588', '-0.0058773844', '-0.5189951007',\n",
       "       'WORLDS OKAYEST MOTHER! &lt', '0.912275923', '0.9710032199',\n",
       "       '0.1056180096', '-0.437514788', '-0.8420397119', '-0.4210260514',\n",
       "       '-0.3876734596', '0.2108946367', '0.4528013332', '-0.2464259483',\n",
       "       '0.6942877028', '-0.0246914482', nan, '0.4344231372',\n",
       "       '0.9399300863', '0.1348003709', '0.9653186592', '-0.6931304197',\n",
       "       '0.8941325952', '0.5209953176', '0.6236458018', '0.9515210511',\n",
       "       '0.7377284677', '-0.3635772458',\n",
       "       ' ereience. My has something I mentioned no e!!', '-0.5106651014',\n",
       "       '0.5131974685', '0.4687557666', '0.4211766825', '0.9006684707',\n",
       "       '0.1666662023', '0.7109585641', '0.6358664785', '0.7312674232',\n",
       "       '0.376611721', '-0.7346278914', '-0.3390036596', '0.8892995982',\n",
       "       '0.831601782',\n",
       "       ' pink Peruvian opal - perfect gift for the woman who loves unique gemstone jewelry! via',\n",
       "       '0.4021564007', '0.7909643914', '0.9178192245', '0.5663170904',\n",
       "       '0.8169043155', ' Find More | ONLINE STORE:', '-0.3410990252',\n",
       "       '0.3604023493', '-0.9373846265', '0.4549097862', '0.7362879641',\n",
       "       'Anger Is The Canary In The Coal Mine Of Emotion..',\n",
       "       '-0.2758448854', '0.7792488606', '0.3352327468',\n",
       "       ' pink Peruvian opal.! via', '-0.507859761', '0.9367567292',\n",
       "       '0.8410259199', '-0.9601591519', '0.9147400439', '0.9555390377',\n",
       "       '0.3342376633', '0.8229897875', '0.8835719655', '-0.4285274298',\n",
       "       '-0.199443947', '0.3663603571', '-0.239295957', '-0.0573958373',\n",
       "       '0.0357543811', '0.7431736126', '-0.3193794251', '0.4757191968',\n",
       "       '0.082564503', '0.9078654625', ' Baby &gt', '-0.0132470091',\n",
       "       '0.5077941309', '-0.5681157577', '0.3779170304', '0.3801804299',\n",
       "       '-0.4029838305', '-0.6792288222', '0.2087168809', '-0.0843953496',\n",
       "       '-0.8186697209', ' are in other areas. My is over &amp',\n",
       "       '-0.8977306127',\n",
       "       ' see the majority of it is filled with purposeful misinformation. Her followers must enjoy having a toothache.',\n",
       "       '0.2186220046', '-0.6659411501', '-0.1867304213', '-0.4797768044',\n",
       "       ' Brand-Safe&gt', '0.13417734', '0.3919858916', '0.9484787256',\n",
       "       '0.7809500796', '0.2411550093', '0.9879367451', '0.2657472421',\n",
       "       ' sent it to my son . He fits the job requirements &amp',\n",
       "       ' visits to . So bought some pasta and pulled out a bottle of wine to cheer us all up! is coming up!',\n",
       "       '-0.6003216828', '0.4148907124', '0.4872389599', '0.1462077484',\n",
       "       '-0.3850425633', '0.6086937311', '0.3161011091', '0.9007707269',\n",
       "       '0.5361733802', '0.3317084053', '0.1486563363', '0.2172353137',\n",
       "       '0.4045569311',\n",
       "       ' ITs over patients. IHope he is in hands. Trust is .',\n",
       "       '0.166229458', '0.0838859069', '-0.4360879956', '0.8276635804',\n",
       "       '0.0432641445', '-0.6046853911', '0.4986807029', '0.063834111',\n",
       "       '0.4502703993', '0.7917092448', '0.6890497729', '-0.8563259346',\n",
       "       '0.5467438684',\n",
       "       ' coloring pages online. Please share to others. Reading with Mrs. K',\n",
       "       '0.8484979256', '-0.1475008928', '0.0436825908', '0.1417574217',\n",
       "       '0.7565479307', '0.2117897904', '-0.8739088126', '0.4945825935',\n",
       "       '0.6927740873', '0.2522315249'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['lang'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en                            2994\n",
       " pink Peruvian opal! via         4\n",
       " Find More                       2\n",
       "&gt                              2\n",
       "WORLDS OKAYEST MOTHER! &lt       2\n",
       "                              ... \n",
       "0.3716244571                     1\n",
       "-0.3410990252                    1\n",
       "-0.4757848717                    1\n",
       "0.3352327468                     1\n",
       "0.8644551076                     1\n",
       "Name: lang, Length: 232, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3235"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['original_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy  MothersDay  love mother to all you amazing mothers  : MothersDay love mother'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting hashtags in a seperate column\n",
    "def get_hashtags(input_str):\n",
    "    #print(input_str)\n",
    "    hash_ls=re.findall(r\"#(\\w+)\", input_str)\n",
    "    #print(hash_ls)\n",
    "    hash_replaced_str=input_str.replace(\"#\",\" \")\n",
    "    #print(hash_replaced_str)\n",
    "    #return [hash_replaced_str,(hash_ls)]\n",
    "    hash_ls_str=\"\"\n",
    "    if len(hash_ls)>0:\n",
    "        hash_ls_str=\" \".join(hash_ls).strip()\n",
    "    \n",
    "    #return pd.Series([hash_replaced_str,hash_ls_str], index=['comment_wo_hash','hashtags'])\n",
    "    return hash_replaced_str+\" : \"+hash_ls_str\n",
    "\n",
    "#vectorizing \n",
    "sample_str=\"Happy #MothersDay #love#mother to all you amazing mothers \"\n",
    "#new_str,hash_ls=get_hashtags(sample_str)\n",
    "#new_str,hash_ls\n",
    "new_str=get_hashtags(sample_str)\n",
    "new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Happy #MothersDay to all you amazing mothers o...\n",
       "1       Happy Mothers Day Mum - I'm sorry I can't be t...\n",
       "2       Happy mothers day To all This doing a mothers ...\n",
       "3       Happy mothers day to this beautiful woman...ro...\n",
       "4       Remembering the 3 most amazing ladies who made...\n",
       "                              ...                        \n",
       "3230    To all my sisters ,my sisters -in -law and als...\n",
       "3231    Happy Mother‚Äôs Day to all the Mums, Step Mums,...\n",
       "3232    Happy Mothers Day to the craziest woman I know...\n",
       "3233    Happy Mother's Day to my amazing wife! We both...\n",
       "3234    Wishing you all a safe & happy Mothers Day #mo...\n",
       "Name: original_text, Length: 3235, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['original_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#input_data[['comment_wo_hash','hashtags']]=input_data.apply(lambda row: get_hashtags(row['original_text']),axis=1)\n",
    "input_data['comment_wo_hash_n_hashtags']=input_data.apply(lambda row: get_hashtags(row['original_text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Happy  MothersDay to all you amazing mothers o...\n",
       "1       Happy Mothers Day Mum - I'm sorry I can't be t...\n",
       "2       Happy mothers day To all This doing a mothers ...\n",
       "3       Happy mothers day to this beautiful woman...ro...\n",
       "4       Remembering the 3 most amazing ladies who made...\n",
       "                              ...                        \n",
       "3230    To all my sisters ,my sisters -in -law and als...\n",
       "3231    Happy Mother‚Äôs Day to all the Mums, Step Mums,...\n",
       "3232    Happy Mothers Day to the craziest woman I know...\n",
       "3233    Happy Mother's Day to my amazing wife! We both...\n",
       "3234    Wishing you all a safe & happy Mothers Day  mo...\n",
       "Name: comment_wo_hash_n_hashtags, Length: 3235, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['comment_wo_hash_n_hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_t=input_data['sentiment_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Train test split==\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(input_data['comment_wo_hash_n_hashtags'].values, y_t, \n",
    "                                                  stratify=y_t, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2911,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(324,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "xvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tf Idf with Logictic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tvf_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 704 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=3, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=1, stop_words='english', strip_accents='unicode',\n",
       "                sublinear_tf=1, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=1, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfv.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train_tvf_comments_hashtags=tfv.transform(xtrain)\n",
    "x_val_tvf_comments_hashtags=tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2911x4383 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 88630 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train_tvf_comments_hashtags=tfv.transform(input_data['comment_wo_hash_n_hashtags'])\n",
    "#x_train_tvf_hashtags=tfv.transform(input_data['hashtags'])\n",
    "x_train_tvf_comments_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<324x4383 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9440 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_tvf_comments_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "G:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train a classifier ==> logistic regressor\n",
    "#simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(x_train_tvf_comments_hashtags, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_val_preds=clf.predict(x_val_tvf_comments_hashtags)\n",
    "#y_val_preds\n",
    "y_predictions=clf.predict_proba(x_val_tvf_comments_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.273 \n"
     ]
    }
   ],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, y_predictions))\n",
    "zx_1=multiclass_logloss(yvalid, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0459777482733583"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "zx=log_loss(yvalid, y_predictions)\n",
    "zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tvf_dict['multiclass_logloss']=zx_1\n",
    "tvf_dict['logloss']=zx\n",
    "accuracy_metrics['Tf_Idf_Linear_regressor']=tvf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.246628e+18</td>\n",
       "      <td>3. Yeah, I once cooked potatoes when I was 3 y...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>LToddWood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.245898e+18</td>\n",
       "      <td>Happy Mother's Day to all the mums, step-mums,...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>iiarushii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.244717e+18</td>\n",
       "      <td>I love the people from the UK, however, when I...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.245730e+18</td>\n",
       "      <td>Happy 81st Birthday Happy Mother‚Äôs Day to my m...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>TheBookTweeters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.244636e+18</td>\n",
       "      <td>Happy Mothers day to all those wonderful mothe...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.246628e+18  3. Yeah, I once cooked potatoes when I was 3 y...   en   \n",
       "1  1.245898e+18  Happy Mother's Day to all the mums, step-mums,...   en   \n",
       "2  1.244717e+18  I love the people from the UK, however, when I...   en   \n",
       "3  1.245730e+18  Happy 81st Birthday Happy Mother‚Äôs Day to my m...   en   \n",
       "4  1.244636e+18  Happy Mothers day to all those wonderful mothe...   en   \n",
       "\n",
       "  retweet_count  original_author  \n",
       "0             0        LToddWood  \n",
       "1             0        iiarushii  \n",
       "2             0   andreaanderegg  \n",
       "3             1  TheBookTweeters  \n",
       "4             0   andreaanderegg  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data['comment_wo_hash_n_hashtags']=test_data.apply(lambda row: get_hashtags(row['original_text']),axis=1)\n",
    "x_test_tvf_comments_hashtags=tfv.transform(test_data['comment_wo_hash_n_hashtags'])\n",
    "#make predictions here\n",
    "y_test=clf.predict_proba(x_test_tvf_comments_hashtags)\n",
    "y_test_pred=clf.predict(x_test_tvf_comments_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39789397, 0.37594803, 0.226158  ],\n",
       "       [0.25979267, 0.48841151, 0.25179581],\n",
       "       [0.21448495, 0.39803834, 0.38747671],\n",
       "       ...,\n",
       "       [0.26814156, 0.60116594, 0.1306925 ],\n",
       "       [0.2658526 , 0.49144289, 0.2427045 ],\n",
       "       [0.28245283, 0.4514692 , 0.26607797]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  0, ...,  0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tf_Idf_Linear_regressor': {'multiclass_logloss': 1.2727885150962162,\n",
       "  'logloss': 1.0459777482733583}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# count Vectorizer with Logistic Rgression (Linear regressor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3), stop_words = 'english')\n",
    "count_vector = CountVectorizer(analyzer='word',ngram_range=(1, 3), stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train_countV=count_vector.transform(xtrain)\n",
    "x_val_countV=count_vector.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "G:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(x_train_countV, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.695 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2357009321133792"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_preds=clf.predict(x_val_countV)\n",
    "#y_val_preds\n",
    "y_predictions=clf.predict_proba(x_val_countV)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, y_predictions))\n",
    "zx_1=multiclass_logloss(yvalid, y_predictions)\n",
    "zx=log_loss(yvalid, y_predictions)\n",
    "zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vec_dict={}\n",
    "count_vec_dict['multiclass_logloss']=zx_1\n",
    "count_vec_dict['logloss']=zx\n",
    "accuracy_metrics['Count_vectorizer_Linear_regressor']=count_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tf_Idf_Linear_regressor': {'multiclass_logloss': 1.2727885150962162,\n",
       "  'logloss': 1.0459777482733583},\n",
       " 'Count_vectorizer_Linear_regressor': {'multiclass_logloss': 1.695275339627342,\n",
       "  'logloss': 1.2357009321133792}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1, ...,  0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_count_vector=count_vector.transform(test_data['comment_wo_hash_n_hashtags'])\n",
    "#make predictions here\n",
    "y_test=clf.predict_proba(x_test_count_vector)\n",
    "y_test_pred=clf.predict(x_test_count_vector)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39837861, 0.32432741, 0.27729398],\n",
       "       [0.24237964, 0.6456318 , 0.11198855],\n",
       "       [0.12238613, 0.23259882, 0.64501505],\n",
       "       ...,\n",
       "       [0.15579101, 0.81238607, 0.03182291],\n",
       "       [0.26619615, 0.62176289, 0.11204096],\n",
       "       [0.32444617, 0.4688647 , 0.20668913]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tf-Idf Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nm_clf=MultinomialNB()\n",
    "nm_clf.fit(x_train_tvf_comments_hashtags, ytrain)\n",
    "predictions = nm_clf.predict_proba(x_val_tvf_comments_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.510 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1096154051858753"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zx_1=multiclass_logloss(yvalid, predictions)\n",
    "print(\"logloss: %0.3f \" % zx_1)\n",
    "zx=log_loss(yvalid, predictions)\n",
    "zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_NB={}\n",
    "tfidf_NB['multiclass_logloss']=zx_1\n",
    "tfidf_NB['logloss']=zx\n",
    "accuracy_metrics['Tf_Idf_Naive_Bayes']=tfidf_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tf_Idf_Linear_regressor': {'multiclass_logloss': 1.2727885150962162,\n",
       "  'logloss': 1.0459777482733583},\n",
       " 'Count_vectorizer_Linear_regressor': {'multiclass_logloss': 1.695275339627342,\n",
       "  'logloss': 1.2357009321133792},\n",
       " 'Tf_Idf_Naive_Bayes': {'multiclass_logloss': 1.5102103818934163,\n",
       "  'logloss': 1.1096154051858753}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Count Vectorizer Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 9.809 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.226935517438707"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cv_clf=MultinomialNB()\n",
    "#x_train_countV\n",
    "#x_val_countV\n",
    "nb_cv_clf.fit(x_train_countV, ytrain)\n",
    "\n",
    "y_val_preds=nb_cv_clf.predict(x_val_countV)\n",
    "#y_val_preds\n",
    "y_predictions=nb_cv_clf.predict_proba(x_val_countV)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, y_predictions))\n",
    "zx_1=multiclass_logloss(yvalid, y_predictions)\n",
    "zx=log_loss(yvalid, y_predictions)\n",
    "zx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CV_NB={}\n",
    "CV_NB['multiclass_logloss']=zx_1\n",
    "CV_NB['logloss']=zx\n",
    "accuracy_metrics['Count_vector_Naive_Bayes']=CV_NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SVM model - simple, without Truncated singular value decomposition and latent semantic analysis\n",
    "\n",
    "Truncated singular value decomposition and latent semantic analysis : https://scikit-learn.org/stable/modules/decomposition.html#lsa\n",
    "\n",
    "When truncated SVD is applied to term-document matrices (as returned by CountVectorizer or TfidfVectorizer), this transformation is known as latent semantic analysis (LSA), because it transforms such matrices to a ‚Äúsemantic‚Äù space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.\n",
    "\n",
    "\n",
    "Note Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix  so that it has shape n_features √ó n_samples. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TF IDF vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2911,)\n",
      "(2911, 4383)\n",
      "Wall time: 10min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svc',\n",
       "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma='auto',\n",
       "                     kernel='rbf', max_iter=-1, probability=True,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "print(ytrain.shape)\n",
    "print(x_train_tvf_comments_hashtags.todense().shape)\n",
    "#Standard Scaler to standardize data, before applying SVM.\n",
    "svm_clf = make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True))\n",
    "svm_clf.fit(x_train_tvf_comments_hashtags.todense(), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2493673873149678 1.019894264732681\n"
     ]
    }
   ],
   "source": [
    "y_predictions_proba=svm_clf.predict_proba(x_val_tvf_comments_hashtags.todense())\n",
    "zx_1=multiclass_logloss(yvalid, y_predictions_proba)\n",
    "zx=log_loss(yvalid, y_predictions_proba)\n",
    "print(zx_1,zx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Tf_Idf_SVM_basic={}\n",
    "Tf_Idf_SVM_basic['multiclass_logloss']=zx_1\n",
    "Tf_Idf_SVM_basic['logloss']=zx\n",
    "accuracy_metrics['Tf_Idf_SVM_basic']=Tf_Idf_SVM_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tf_Idf_Linear_regressor': {'multiclass_logloss': 1.2727885150962162,\n",
       "  'logloss': 1.0459777482733583},\n",
       " 'Count_vectorizer_Linear_regressor': {'multiclass_logloss': 1.695275339627342,\n",
       "  'logloss': 1.2357009321133792},\n",
       " 'Tf_Idf_Naive_Bayes': {'multiclass_logloss': 1.5102103818934163,\n",
       "  'logloss': 1.1096154051858753},\n",
       " 'Count_vector_Naive_Bayes': {'multiclass_logloss': 9.809364848044815,\n",
       "  'logloss': 6.226935517438707},\n",
       " 'Tf_Idf_SVM_basic': {'multiclass_logloss': 1.2493673873149678,\n",
       "  'logloss': 1.019894264732681}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Count vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2911, 100708), (2911,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_countV.shape , ytrain.shape # x_train_countV, ytrain\n",
    "#x_val_countV yvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "%%time\n",
    "'''taking too much time to run. Will add max_iter and verbose as algo may not be converging '''\n",
    "\n",
    "svm_clf_cv = make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True))\n",
    "svm_clf_cv.fit(x_train_countV.todense(), ytrain)\n",
    "#nb_cv_clf.fit(x_train_countV, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "%%time\n",
    "y_val_preds=svm_clf_cv.predict_proba(x_val_countV.todense())\n",
    "#y_val_preds\n",
    "#y_predictions=nb_cv_clf.predict_proba(x_val_countV)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, y_val_preds))\n",
    "zx_1=multiclass_logloss(yvalid, y_val_preds)\n",
    "zx=log_loss(yvalid, y_val_preds)\n",
    "zx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TruncatedSVM model - simple Truncated singular value decomposition and latent semantic analysis + Strandadized Data\n",
    "\n",
    "\n",
    "Truncating svm does not cause a significant decrease in model performance, but a very high improvement in execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 939 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#print(ytrain.shape)\n",
    "#print(x_train_tvf_comments_hashtags.todense().shape)\n",
    "\n",
    "#x_train_tvf_comments_hashtags=tfv.transform(xtrain)\n",
    "#x_val_tvf_comments_hashtags=tfv.transform(xvalid)\n",
    "\n",
    "#Standard Scaler to standardize data, before applying SVM.\n",
    "#svm_clf = make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True))\n",
    "#svm_clf.fit(x_train_tvf_comments_hashtags.todense(), ytrain)\n",
    "\n",
    "\n",
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=150)\n",
    "svd.fit(x_train_tvf_comments_hashtags)\n",
    "\n",
    "#transforming the Tf-IDF vectors to svd vectors.\n",
    "xtrain_svd = svd.transform(x_train_tvf_comments_hashtags)\n",
    "xvalid_svd = svd.transform(x_val_tvf_comments_hashtags)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.248 \n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2475833991381615 1.0201701849581735\n"
     ]
    }
   ],
   "source": [
    "zx_1=multiclass_logloss(yvalid, predictions)\n",
    "zx=log_loss(yvalid, predictions)\n",
    "print(zx_1,zx)\n",
    "\n",
    "Tf_Idf_trunc_SVM_LSA={}\n",
    "Tf_Idf_trunc_SVM_LSA['multiclass_logloss']=zx_1\n",
    "Tf_Idf_trunc_SVM_LSA['logloss']=zx\n",
    "accuracy_metrics['Tf_Idf_trunc_SVM_LSA']=Tf_Idf_trunc_SVM_LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svd_cv = decomposition.TruncatedSVD(n_components=150)\n",
    "svd_cv.fit(x_train_countV)\n",
    "\n",
    "\n",
    "#x_train_countV=count_vector.transform(xtrain)\n",
    "#x_val_countV=count_vector.transform(xvalid)\n",
    "\n",
    "#transforming the Tf-IDF vectors to svd vectors.\n",
    "xtrain_svd_cv = svd_cv.transform(x_train_countV)\n",
    "xvalid_svd_cv = svd_cv.transform(x_val_countV)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl_cv = preprocessing.StandardScaler()\n",
    "scl_cv.fit(xtrain_svd_cv)\n",
    "xtrain_svd_scl_cv = scl_cv.transform(xtrain_svd_cv)\n",
    "xvalid_svd_scl_cv = scl_cv.transform(xvalid_svd_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.249 \n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_svm_cv = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf_svm_cv.fit(xtrain_svd_scl_cv, ytrain)\n",
    "predictions = clf_svm_cv.predict_proba(xvalid_svd_scl_cv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.249167774079908 1.0216142932120083\n"
     ]
    }
   ],
   "source": [
    "zx_1=multiclass_logloss(yvalid, predictions)\n",
    "zx=log_loss(yvalid, predictions)\n",
    "print(zx_1,zx)\n",
    "\n",
    "cv_trunc_SVM_LSA={}\n",
    "cv_trunc_SVM_LSA['multiclass_logloss']=zx_1\n",
    "cv_trunc_SVM_LSA['logloss']=zx\n",
    "accuracy_metrics['CV_trunc_SVM_LSA']=cv_trunc_SVM_LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tf_Idf_Linear_regressor': {'multiclass_logloss': 1.2727885150962162,\n",
       "  'logloss': 1.0459777482733583},\n",
       " 'Count_vectorizer_Linear_regressor': {'multiclass_logloss': 1.695275339627342,\n",
       "  'logloss': 1.2357009321133792},\n",
       " 'Tf_Idf_Naive_Bayes': {'multiclass_logloss': 1.5102103818934163,\n",
       "  'logloss': 1.1096154051858753},\n",
       " 'Count_vector_Naive_Bayes': {'multiclass_logloss': 9.809364848044815,\n",
       "  'logloss': 6.226935517438707},\n",
       " 'Tf_Idf_SVM_basic': {'multiclass_logloss': 1.2493673873149678,\n",
       "  'logloss': 1.019894264732681},\n",
       " 'Tf_Idf_trunc_SVM_LSA': {'multiclass_logloss': 1.2475833991381615,\n",
       "  'logloss': 1.0201701849581735},\n",
       " 'CV_trunc_SVM_LSA': {'multiclass_logloss': 1.249167774079908,\n",
       "  'logloss': 1.0216142932120083}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Fitting a simple xgboost on tf-idf\n",
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf.fit(x_train_tvf_comments_hashtags.tocsc(), ytrain)\n",
    "#tocsc() :: Compressed Sparse Column format\n",
    "predictions = xgb_clf.predict_proba(x_val_tvf_comments_hashtags.tocsc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3760362689233856 1.0965642127909778\n"
     ]
    }
   ],
   "source": [
    "zx_1=multiclass_logloss(yvalid, predictions)\n",
    "zx=log_loss(yvalid, predictions)\n",
    "print(zx_1,zx)\n",
    "\n",
    "Tf_Idf_XGB={}\n",
    "Tf_Idf_XGB['multiclass_logloss']=zx_1\n",
    "Tf_Idf_XGB['logloss']=zx\n",
    "accuracy_metrics['Tf_Idf_XGB']=Tf_Idf_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # Fitting a simple xgboost on tf-idf svd features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.884 \n",
      "1.8838025765424524 1.3058248953606335\n"
     ]
    }
   ],
   "source": [
    "xgb_clf_svd = xgb.XGBClassifier(nthread=10)\n",
    "#Unscaled\n",
    "#xtrain_svd\n",
    "xgb_clf_svd.fit(xtrain_svd, ytrain)\n",
    "predictions = xgb_clf_svd.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "zx_1=multiclass_logloss(yvalid, predictions)\n",
    "zx=log_loss(yvalid, predictions)\n",
    "print(zx_1,zx)\n",
    "\n",
    "xgb_svd_tfidf={}\n",
    "xgb_svd_tfidf['multiclass_logloss']=zx_1\n",
    "xgb_svd_tfidf['logloss']=zx\n",
    "accuracy_metrics['xgb_svd_tfidf']=xgb_svd_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # Fitting a simple xgboost on tf-idf svd features _Scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.884 \n",
      "1.8838025765424524 1.3058248953606335\n"
     ]
    }
   ],
   "source": [
    "xgb_clf_svd_scaled = xgb.XGBClassifier(nthread=10)\n",
    "#Scaled\n",
    "#xtrain_svd\n",
    "xgb_clf_svd_scaled.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = xgb_clf_svd_scaled.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "zx_1=multiclass_logloss(yvalid, predictions)\n",
    "zx=log_loss(yvalid, predictions)\n",
    "print(zx_1,zx)\n",
    "\n",
    "xgb_svd_tfidf_scaled={}\n",
    "xgb_svd_tfidf_scaled['multiclass_logloss']=zx_1\n",
    "xgb_svd_tfidf_scaled['logloss']=zx\n",
    "accuracy_metrics['xgb_svd_tfidf_scaled']=xgb_svd_tfidf_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_cv = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf_cv.fit(x_train_countV.tocsc(), ytrain)\n",
    "#tocsc() :: Compressed Sparse Column format\n",
    "predictions_cv = xgb_clf_cv.predict_proba(x_val_countV.tocsc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3683314268235807 1.0621958973783034\n"
     ]
    }
   ],
   "source": [
    "zx_1=multiclass_logloss(yvalid, predictions_cv)\n",
    "zx=log_loss(yvalid, predictions_cv)\n",
    "print(zx_1,zx)\n",
    "\n",
    "cv_XGB={}\n",
    "cv_XGB['multiclass_logloss']=zx_1\n",
    "cv_XGB['logloss']=zx\n",
    "accuracy_metrics['Tf_Idf_XGB']=cv_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # Fitting a simple xgboost on cv svd features unscaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.869 \n",
      "1.868590195858736 1.3226814595407541\n"
     ]
    }
   ],
   "source": [
    "#xtrain_svd_cv\n",
    "#xvalid_svd_cv\n",
    "\n",
    "xgb_clf_svd_cv = xgb.XGBClassifier(nthread=10)\n",
    "#Un-scaled\n",
    "#xtrain_svd\n",
    "xgb_clf_svd_cv.fit(xtrain_svd_cv, ytrain)\n",
    "predictions = xgb_clf_svd_cv.predict_proba(xvalid_svd_cv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "zx_1=multiclass_logloss(yvalid, predictions)\n",
    "zx=log_loss(yvalid, predictions)\n",
    "print(zx_1,zx)\n",
    "\n",
    "xgb_svd_cv={}\n",
    "xgb_svd_cv['multiclass_logloss']=zx_1\n",
    "xgb_svd_cv['logloss']=zx\n",
    "accuracy_metrics['xgb_svd_cv']=xgb_svd_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # Fitting a simple xgboost on cv svd features unscaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.894 \n",
      "1.893945956022053 1.3612558731153883\n"
     ]
    }
   ],
   "source": [
    "xgb_clf_svd_cv_scaled = xgb.XGBClassifier(nthread=10)\n",
    "#Scaled\n",
    "#xtrain_svd\n",
    "xgb_clf_svd_cv_scaled.fit(xtrain_svd_scl_cv, ytrain)\n",
    "predictions = xgb_clf_svd_cv_scaled.predict_proba(xvalid_svd_scl_cv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "zx_1=multiclass_logloss(yvalid, predictions)\n",
    "zx=log_loss(yvalid, predictions)\n",
    "print(zx_1,zx)\n",
    "\n",
    "xgb_svd_cv_scaled={}\n",
    "xgb_svd_cv_scaled['multiclass_logloss']=zx_1\n",
    "xgb_svd_cv_scaled['logloss']=zx\n",
    "accuracy_metrics['xgb_svd_cv_scaled']=xgb_svd_cv_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter optimizations on XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SCORING FUNCTION\n",
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-Idf grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:    5.8s remaining:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:    5.9s remaining:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:    6.0s remaining:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:    6.9s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:    7.2s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:    7.4s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:    7.9s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -1.259\n",
      "Best parameters set:\n",
      "\tlr__C: 0.1\n",
      "\tlr__penalty: 'l1'\n",
      "\tsvd__n_components: 120\n",
      "Wall time: 8.94 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "G:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(x_train_tvf_comments_hashtags, ytrain)  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 7 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  42 | elapsed:   20.1s remaining:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  42 | elapsed:   20.2s remaining:    9.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -1.283\n",
      "Best parameters set:\n",
      "\tnb__alpha: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 out of  42 | elapsed:   20.3s remaining:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  42 | elapsed:   20.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:   20.5s finished\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100,250]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=6)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(x_train_tvf_comments_hashtags, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313555it [01:33, 3363.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-d7d404354375>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'G:\\Projects\\Glove Wectors\\glove.6B.300d.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m                         \u001b[1;31m# If no `miniters` was specified, adjust automatically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1336\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mdisplay\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1470\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mprint_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mfp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[0mfp_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\AppsNsoftwares\\Anaconda\\envs\\test_env\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('G:\\Projects\\Glove Wectors\\glove.6B.300d.txt',encoding=\"utf-8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testing Env)",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
